{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the trained filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some startup! \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# This is needed to save images \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model saved by train.py\n",
    "# This will be an instance of models.softmax.Softmax.\n",
    "# NOTE: You may need to change this file name.\n",
    "\n",
    "# model = torch.load('softmax.pt'); model_name = \"softmax\";\n",
    "# model = torch.load('twolayernn.pt'); model_name = \"twolayernn\";\n",
    "# model = torch.load('convnet.pt'); model_name = \"convnet\";\n",
    "model = torch.load('mymodel.pt'); model_name = \"mymodel\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[10, 32, 32, 3]' is invalid for input of size 81920",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a18e177f5db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#for softmax and twolayernn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mw_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 32, 32, 3]' is invalid for input of size 81920"
     ]
    }
   ],
   "source": [
    "# collect all the weights\n",
    "w = None\n",
    "#############################################################################\n",
    "# TODO: Extract the weight matrix (without bias) from softmax_model, convert\n",
    "# it to a numpy array with shape (10, 32, 32, 3), and assign this array to w.\n",
    "# The first dimension should be for channels, then height, width, and color.\n",
    "# This step depends on how you implemented models.softmax.Softmax.\n",
    "#############################################################################\n",
    "#print(softmax_model)\n",
    "#print(softmax_model.linear.weight.shape)\n",
    "w_tensor = model.fc1.weight\n",
    "b_tensor = model.fc1.bias #not used\n",
    "sizes = w_tensor.size()\n",
    "'''\n",
    "print(f'w_tensor shape = {sizes}')\n",
    "print(w_tensor[0][0])\n",
    "print(w_tensor[0][1])\n",
    "print(w_tensor[0][2])\n",
    "\n",
    "print(w_tensor[0][3069])\n",
    "print(w_tensor[0][3070])\n",
    "print(w_tensor[0][3071])\n",
    "'''\n",
    "#for softmax and twolayernn\n",
    "#'''\n",
    "w_tensor = w_tensor.view(sizes[0], 32, 32, 3)\n",
    "w = w_tensor.detach().numpy()\n",
    "#'''\n",
    "\n",
    "#for CNN\n",
    "'''\n",
    "w_tensor = w_tensor.view(10, 32, 32, 8)\n",
    "w_temp = w_tensor.detach().numpy()\n",
    "print(f'w shape = {w_temp.shape}')\n",
    "w = np.zeros((10, 32, 32, 3))\n",
    "for i in range(10):\n",
    "    for j in range(32):\n",
    "        for k in range(32):\n",
    "            for l in range(3):\n",
    "                w[i][j][k][l] = w_temp[i][j][k][l]\n",
    "'''\n",
    "'''\n",
    "print(f'w shape = {w.shape}')\n",
    "print(w[0][0])\n",
    "print(w[0][2])\n",
    "'''\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "# obtain min,max to normalize\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "# classes\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# init figure \n",
    "fig = plt.figure(figsize=(6,6))\n",
    "for i in range(10):\n",
    "    wimg = 255.0*(w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "    # subplot is (2,5) as ten filters are to be visualized\n",
    "    fig.add_subplot(2,5,i+1).imshow(wimg.astype('uint8'))\n",
    "# save fig! \n",
    "fig.savefig(f'{model_name}_filt.png')\n",
    "print('figure saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a796bd71d0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# saving the weights is now as simple as:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{model_name}_gridfilt.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvisualize_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# padding is the space between images. Make sure that w is of shape: (N,H,W,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure saved as a grid!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reagan_GT/spring2020/cs4803/assignment1/assignment/2_pytorch/vis_utils.py\u001b[0m in \u001b[0;36mvisualize_grid\u001b[0;34m(Xs, ubound, padding)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;34m-\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mblank\u001b[0m \u001b[0mpixels\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mgrid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mgrid_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrid_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# vis_utils.py has helper code to view multiple filters in single image. Use this to visuzlize \n",
    "# neural network adn convnets. \n",
    "# import vis_utils\n",
    "from vis_utils import visualize_grid\n",
    "# saving the weights is now as simple as:\n",
    "plt.imsave(f'{model_name}_gridfilt.png',visualize_grid(w, padding=3).astype('uint8'))\n",
    "# padding is the space between images. Make sure that w is of shape: (N,H,W,C)\n",
    "print('figure saved as a grid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
